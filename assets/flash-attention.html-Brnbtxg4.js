import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,d as n,o as e}from"./app-alnVQ7pP.js";const i={};function l(m,s){return e(),t("div",null,[...s[0]||(s[0]=[n('<p><strong>一种通过“减少 HBM 访问次数”来加速计算的算法。它不改变计算量（FLOPS），但极大地减少了数据搬运的时间（IO）</strong></p><p>简单一句话总结 FlashAttention 的核心思想：<br><strong>它是一种通过“减少 HBM 访问次数”来加速计算的算法。它不改变计算量（FLOPS），但极大地减少了数据搬运的时间（IO）。</strong></p><p>在大模型领域，<strong>IO（搬运数据）往往比计算更慢</strong>，所以“少跑几趟仓库”就能带来巨大的速度提升。</p><hr><h3 id="_1-传统-attention-的痛点-hbm-的-反复折返跑" tabindex="-1"><a class="header-anchor" href="#_1-传统-attention-的痛点-hbm-的-反复折返跑"><span>1. 传统 Attention 的痛点：HBM 的“反复折返跑”</span></a></h3><p>在标准的 Attention 算法（即 Transformers 的核心公式 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Softmax</mtext><mo stretchy="false">(</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\\text{Softmax}(QK^T)V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Softmax</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>）中，计算过程是非常“铺张浪费”的。</p><p>假设我们有一个长文本序列，长度为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>。</p><ol><li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>×</mo><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">Q \\times K^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>：</strong> GPU 把 Q 和 K 从 HBM 读进 SRAM 算乘法，生成一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N \\times N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 的巨大矩阵（Attention Score）。 <ul><li><em>痛点：</em> SRAM 太小装不下这个大矩阵，所以算完必须<strong>写回 HBM</strong>。</li></ul></li><li><strong>Softmax：</strong> GPU 把刚才那个大矩阵从 HBM 读回来，做 Softmax 归一化。 <ul><li><em>痛点：</em> 算完又要<strong>写回 HBM</strong>。</li></ul></li><li><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>×</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\\times V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord">×</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>：</strong> GPU 再次把归一化后的矩阵和 V 从 HBM 读出来，做最后乘法。 <ul><li><em>痛点：</em> 算完结果<strong>写回 HBM</strong>。</li></ul></li></ol><p><strong>结果：</strong></p><ul><li><strong>显存爆炸：</strong> 那个中间产生的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N \\times N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 矩阵随着序列长度变长，体积呈平方级增长（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>）。如果你输入 100k 字的文档，这个中间矩阵直接把显存撑爆（OOM）。</li><li><strong>带宽瓶颈：</strong> 数据在 HBM 和 SRAM 之间来回搬运了 3 次以上。GPU 核心大部分时间在等 HBM 搬数据，而不是在计算。</li></ul><hr><h3 id="_2-flashattention-的魔法-切块-tiling-与-算子融合" tabindex="-1"><a class="header-anchor" href="#_2-flashattention-的魔法-切块-tiling-与-算子融合"><span>2. FlashAttention 的魔法：切块（Tiling）与 算子融合</span></a></h3><p>FlashAttention 的发明者（Tri Dao）意识到：<strong>既然 SRAM 虽小但极快，那我们为什么不把所有事情都在 SRAM 里一次性做完再出来？</strong></p><h4 id="核心操作-a-分块计算-tiling" tabindex="-1"><a class="header-anchor" href="#核心操作-a-分块计算-tiling"><span>核心操作 A：分块计算 (Tiling)</span></a></h4><p>FlashAttention 不会试图一次性算出整个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N \\times N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 的大矩阵。<br> 它把 Q, K, V 切成很多小块（Block）。比如，每次只切一小块 Q 和一小块 K 放进 SRAM。</p><h4 id="核心操作-b-算子融合-kernel-fusion" tabindex="-1"><a class="header-anchor" href="#核心操作-b-算子融合-kernel-fusion"><span>核心操作 B：算子融合 (Kernel Fusion)</span></a></h4><p>在 SRAM 内部，它一口气完成以下动作：</p><ol><li>算一小块 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>×</mo><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">Q \\times K^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>。</li><li><strong>立即</strong>在 SRAM 里对这一小块数据做 Softmax（配合 Online Softmax 技术，下面会讲）。</li><li><strong>立即</strong>用这一小块结果去乘 V。</li><li>只把最终计算好的那<strong>一小部分结果</strong>写回 HBM。</li></ol><p><strong>关键改变：</strong> 那个巨大的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N \\times N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 中间矩阵根本不需要在 HBM 里生成和存储！它被“拆碎”在 SRAM 里消化掉了。</p><hr><h3 id="_3-数学上的难点-online-softmax" tabindex="-1"><a class="header-anchor" href="#_3-数学上的难点-online-softmax"><span>3. 数学上的难点：Online Softmax</span></a></h3><p>你可能会问：<em>“Softmax 不是需要看全行数据才能算概率吗？切成小块怎么算？”</em></p><p>比如一行数据是 <code>[2, 3, 5]</code>。</p><ul><li>传统 Softmax：必须知道最大值是 5，总和是多少，才能算出每个数的概率。如果只看第一块 <code>[2]</code>，根本不知道后面还有个 <code>5</code>。</li></ul><p><strong>FlashAttention 引入了“Online Softmax”技巧：</strong></p><ul><li>它在处理第一块时，先假设当前看到的是最大值，算一个临时结果。</li><li>当读到第二块发现更大的值时，利用数学公式对之前的临时结果进行<strong>修正（Rescaling）</strong>。</li><li>这样就可以一边读块，一边更新结果，不需要一次性看到全局。</li></ul><hr><h3 id="_4-对-推理-的具体优化作用" tabindex="-1"><a class="header-anchor" href="#_4-对-推理-的具体优化作用"><span>4. 对“推理”的具体优化作用</span></a></h3><p>大模型推理分为两个阶段，FlashAttention 的作用略有不同：</p><h4 id="第一阶段-prefill-预填充-处理提示词" tabindex="-1"><a class="header-anchor" href="#第一阶段-prefill-预填充-处理提示词"><span>第一阶段：Prefill（预填充/处理提示词）</span></a></h4><p>这是你把一大段 Prompt 发给模型，模型阅读理解的阶段。</p><ul><li><strong>场景特点：</strong> 这是一个高度并行的矩阵计算，和训练过程很像。</li><li><strong>FlashAttention 的作用：</strong> <strong>极度显著</strong>。 <ul><li><strong>速度：</strong> 比如你发了 10 万字的文档让 Kimi/GPT-4 总结。没有 FlashAttention，处理这 10 万字可能需要几分钟；有了它，只需几秒。因为它避免了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N \\times N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 矩阵的读写。</li><li><strong>显存：</strong> 使得长文本推理成为可能。它把显存占用从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 降到了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span>（线性增长）。如果没有它，消费级显卡根本跑不了长窗口模型。</li></ul></li></ul><h4 id="第二阶段-decoding-解码-逐字生成" tabindex="-1"><a class="header-anchor" href="#第二阶段-decoding-解码-逐字生成"><span>第二阶段：Decoding（解码/逐字生成）</span></a></h4><p>这是模型一个字一个字往外蹦的阶段。</p><ul><li><strong>场景特点：</strong> 主要是向量乘矩阵（Vector-Matrix Multiply）。每次只生成 1 个 Token，Q 只有 1 行，但 KV Cache 很长。</li><li><strong>FlashAttention 的作用：</strong> <strong>FlashDecoding</strong>。 <ul><li>早期的 FlashAttention 在这里主要优化了显存占用。</li><li>后来的变体 <strong>FlashDecoding</strong> 专门针对推理生成阶段优化：它通过并行加载 KV Cache 的不同部分来加速 Attention 计算。</li><li><strong>结果：</strong> 让生成速度（Tokens/s）在长上下文时不掉速。以前 context 越长生成越慢，现在基本能保持恒定高速。</li></ul></li></ul><hr><h3 id="_5-厨房类比-终极版" tabindex="-1"><a class="header-anchor" href="#_5-厨房类比-终极版"><span>5. 厨房类比（终极版）</span></a></h3><p>为了对应之前的 SRAM/HBM 概念：</p><ul><li><p><strong>做一道极其复杂的菜（计算 Attention）：</strong> 需要切菜、焯水、过油、勾芡。</p></li><li><p><strong>传统做法（Standard Attention）：</strong></p><ol><li>从冰箱（HBM）拿出 100 斤土豆。</li><li>切完，因为案板（SRAM）放不下，把 100 斤切好的土豆放回冰箱。</li><li>从冰箱拿出切好的土豆，焯水，再放回冰箱。</li><li>从冰箱拿出焯水的土豆，过油……</li></ol><ul><li><strong>问题：</strong> 厨师大部分时间都在跑腿开冰箱门，而且冰箱快塞不下了。</li></ul></li><li><p><strong>FlashAttention 做法：</strong></p><ol><li>从冰箱只拿 1 个土豆。</li><li>在案板上（SRAM）瞬间完成切、焯、炒、勾芡。</li><li>直接把这一小盘成品端上桌（写入最终结果）。</li><li>再去拿下一个土豆。</li></ol><ul><li><strong>优势：</strong> 厨师不用跑腿，冰箱也不会被半成品塞满。</li></ul></li></ul><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h3><p>FlashAttention 对大模型推理的贡献在于：</p><ol><li><strong>打破了内存墙：</strong> 通过减少 HBM 访问，极大提升了推理速度（尤其是长文本处理）。</li><li><strong>拯救了显存：</strong> 不再存储巨大的中间矩阵，让有限的显存能处理几十倍长度的 Context Window（比如从 4k 扩展到 128k 甚至 1M）。</li></ol>',42)])])}const r=a(i,[["render",l]]),h=JSON.parse('{"path":"/blogs/flash-attention.html","title":"flash attention","lang":"zh-CN","frontmatter":{"title":"flash attention","date":"2025-12-07T00:00:00.000Z","category":"博客","description":"一种通过“减少 HBM 访问次数”来加速计算的算法。它不改变计算量（FLOPS），但极大地减少了数据搬运的时间（IO） 简单一句话总结 FlashAttention 的核心思想： 它是一种通过“减少 HBM 访问次数”来加速计算的算法。它不改变计算量（FLOPS），但极大地减少了数据搬运的时间（IO）。 在大模型领域，IO（搬运数据）往往比计算更慢，所...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"flash attention\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-12-07T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-07T10:02:00.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"guoziyue-seeker\\",\\"url\\":\\"https://github.com/guoziyue-seeker\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/llm-guide/blogs/flash-attention.html"}],["meta",{"property":"og:site_name","content":"LLMGuide"}],["meta",{"property":"og:title","content":"flash attention"}],["meta",{"property":"og:description","content":"一种通过“减少 HBM 访问次数”来加速计算的算法。它不改变计算量（FLOPS），但极大地减少了数据搬运的时间（IO） 简单一句话总结 FlashAttention 的核心思想： 它是一种通过“减少 HBM 访问次数”来加速计算的算法。它不改变计算量（FLOPS），但极大地减少了数据搬运的时间（IO）。 在大模型领域，IO（搬运数据）往往比计算更慢，所..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-07T10:02:00.000Z"}],["meta",{"property":"article:published_time","content":"2025-12-07T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-07T10:02:00.000Z"}]]},"git":{"createdTime":1765098988000,"updatedTime":1765101720000,"contributors":[{"name":"guoziyue666","username":"guoziyue666","email":"2567047574@qq.com","commits":2,"url":"https://github.com/guoziyue666"}]},"readingTime":{"minutes":5.07,"words":1522},"filePathRelative":"blogs/flash-attention.md","excerpt":"<p><strong>一种通过“减少 HBM 访问次数”来加速计算的算法。它不改变计算量（FLOPS），但极大地减少了数据搬运的时间（IO）</strong></p>\\n<p>简单一句话总结 FlashAttention 的核心思想：<br>\\n<strong>它是一种通过“减少 HBM 访问次数”来加速计算的算法。它不改变计算量（FLOPS），但极大地减少了数据搬运的时间（IO）。</strong></p>\\n<p>在大模型领域，<strong>IO（搬运数据）往往比计算更慢</strong>，所以“少跑几趟仓库”就能带来巨大的速度提升。</p>\\n<hr>\\n<h3>1. 传统 Attention 的痛点：HBM 的“反复折返跑”</h3>","autoDesc":true}');export{r as comp,h as data};
