import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,d as n,o as m}from"./app-DQLVeGHL.js";const e={};function p(i,s){return m(),t("div",null,[...s[0]||(s[0]=[n('<p><strong>MLA (Multi-Head Latent Attention，多头潜在注意力机制)</strong> 是由 <strong>DeepSeek (深度求索)</strong> 在其 <strong>DeepSeek-V2</strong> 模型中提出的一种创新架构。</p><hr><h3 id="_1-背景-显存的瓶颈-kv-cache" tabindex="-1"><a class="header-anchor" href="#_1-背景-显存的瓶颈-kv-cache"><span>1. 背景：显存的瓶颈 (KV Cache)</span></a></h3><p>在理解这些机制的区别前，核心要关注的指标是 <strong>KV Cache（键值缓存）</strong>。<br> 在大模型推理（Generate）阶段，为了避免重复计算，我们会把之前生成的 Token 的 Key 和 Value 矩阵存在显存里。</p><p>随着上下文（Context Length）变长、Batch Size 变大，KV Cache 会占用巨大的显存，甚至超过模型权重本身，导致显存不足（OOM）或吞吐量下降。</p><hr><h3 id="_2-演进之路-mha-vs-gqa" tabindex="-1"><a class="header-anchor" href="#_2-演进之路-mha-vs-gqa"><span>2. 演进之路：MHA vs GQA</span></a></h3><h4 id="mha-multi-head-attention-多头注意力" tabindex="-1"><a class="header-anchor" href="#mha-multi-head-attention-多头注意力"><span><strong>MHA (Multi-Head Attention，多头注意力)</strong></span></a></h4><ul><li><strong>机制</strong>：标准的 Transformer 架构。假设有 32 个头（Head），每个头都有自己独立的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Q, K, V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 投影矩阵。</li><li><strong>KV Cache</strong>：每个头都需要存储独立的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>。</li><li><strong>优点</strong>：模型表达能力最强，因为每个头都能捕捉不同的语义信息。</li><li><strong>缺点</strong>：<strong>显存占用巨大</strong>。 <ul><li><em>公式</em>：Cache 大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∝</mo></mrow><annotation encoding="application/x-tex">\\propto</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mrel">∝</span></span></span></span> 层数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">\\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">×</span></span></span></span> 头数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>×</mo></mrow><annotation encoding="application/x-tex">H \\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord">×</span></span></span></span> 维度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span>。</li></ul></li></ul><h4 id="gqa-grouped-query-attention-分组查询注意力" tabindex="-1"><a class="header-anchor" href="#gqa-grouped-query-attention-分组查询注意力"><span><strong>GQA (Grouped-Query Attention，分组查询注意力)</strong></span></a></h4><ul><li><strong>机制</strong>：为了解决 MHA 显存太大的问题，GQA 将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span> 头分成几组，每组共享同一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>。 <ul><li>例如：32 个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span> 头，每 8 个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span> 共享 1 个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">K, V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>。那么实际上只有 4 个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">K, V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 头。</li></ul></li><li><strong>KV Cache</strong>：显存占用降低为 MHA 的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>G</mi></mrow><annotation encoding="application/x-tex">1/G</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1/</span><span class="mord mathnormal">G</span></span></span></span>（G为分组比例）。</li><li><strong>优点</strong>：显著降低了显存，推理速度快（因为内存带宽需求小）。目前是 LLaMA 3、Mistral 等主流模型的标配。</li><li><strong>缺点</strong>：<strong>主要是有损压缩</strong>。因为强行让多个头共享 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>，损失了一定的模型表达能力（虽然在很多任务上微乎其微，但理论上限确实降低了）。</li></ul><hr><h3 id="_3-mla-multi-head-latent-attention-的改进" tabindex="-1"><a class="header-anchor" href="#_3-mla-multi-head-latent-attention-的改进"><span>3. MLA (Multi-Head Latent Attention) 的改进</span></a></h3><p>MLA 的核心目标是：<strong>拥有 MHA 的性能（强大的表达能力），同时拥有 MQA/GQA 的推理效率（极小的 KV Cache）。</strong></p><p>它是如何做到的？答案是：<strong>低秩压缩 (Low-Rank Compression) + 矩阵吸收 (Matrix Absorption)。</strong></p><h4 id="改进点一-kv-的低秩压缩-极致的显存优化" tabindex="-1"><a class="header-anchor" href="#改进点一-kv-的低秩压缩-极致的显存优化"><span><strong>改进点一：KV 的低秩压缩 (极致的显存优化)</strong></span></a></h4><p>在 MHA 中，我们需要存储完整的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">d_{model}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 维度的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>。<br> MLA 认为，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 中包含大量冗余信息。因此，它不直接存储 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>，而是将输入的向量投影到一个<strong>低维的潜在向量 (Latent Vector)</strong> 中。</p><ul><li><strong>压缩</strong>：输入向量 -&gt; 下投影矩阵 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>D</mi><mi>K</mi><mi>V</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{DKV}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">DK</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>) -&gt; <strong>压缩的潜在向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>K</mi><mi>V</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{KV}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></strong>。</li><li><strong>还原</strong>：压缩的潜在向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>K</mi><mi>V</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{KV}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> -&gt; 上投影矩阵 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>U</mi><mi>K</mi><mi>V</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{UKV}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>) -&gt; 恢复成多头的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>。</li></ul><p><strong>关键的“魔法”在于推理阶段：</strong><br> 如果我们只是压缩再还原，那显存里还是要存还原后的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> 吗？<strong>不需要！</strong></p><p>利用矩阵乘法的结合律，MLA 将“还原矩阵 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>U</mi><mi>K</mi><mi>V</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{UKV}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>)”<strong>吸收</strong>到了 Query (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span>) 的投影矩阵中。</p><ul><li><strong>原本的 Attention</strong>： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>×</mo><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">Q \\times K^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></li><li><strong>MLA 的逻辑</strong>： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>×</mo><mo stretchy="false">(</mo><msub><mi>C</mi><mrow><mi>K</mi><mi>V</mi></mrow></msub><mo>×</mo><msub><mi>W</mi><mrow><mi>U</mi><mi>K</mi><mi>V</mi></mrow></msub><msup><mo stretchy="false">)</mo><mi>T</mi></msup><mo>=</mo><mo stretchy="false">(</mo><mi>Q</mi><mo>×</mo><msubsup><mi>W</mi><mrow><mi>U</mi><mi>K</mi><mi>V</mi></mrow><mi>T</mi></msubsup><mo stretchy="false">)</mo><mo>×</mo><msubsup><mi>C</mi><mrow><mi>K</mi><mi>V</mi></mrow><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">Q \\times (C_{KV} \\times W_{UKV})^T = (Q \\times W_{UKV}^T) \\times C_{KV}^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1167em;vertical-align:-0.2753em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4247em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">U</span><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2753em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1167em;vertical-align:-0.2753em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4247em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2753em;"><span></span></span></span></span></span></span></span></span></span></li></ul><p><strong>结果</strong>：我们在推理时的 KV Cache 只需要存储那个<strong>极小的压缩向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>K</mi><mi>V</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{KV}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></strong>。<br> 这使得 MLA 的 KV Cache 大小极小（通常甚至比 MQA 还要小，或者是 GQA 的几分之一），但它在数学上等价于拥有很多个头，因此保留了 MHA 的多头能力。</p><h4 id="改进点二-decoupled-rope-解耦的位置编码" tabindex="-1"><a class="header-anchor" href="#改进点二-decoupled-rope-解耦的位置编码"><span><strong>改进点二：Decoupled RoPE (解耦的位置编码)</strong></span></a></h4><p>这是 MLA 为了配合上述压缩策略必须做的改进。<br> 通常的 RoPE（旋转位置编码）是直接作用在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 上的。如果 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 被 RoPE 旋转了，矩阵的相对关系就被破坏了，上面的“矩阵吸收”技巧就失效了（因为旋转是非线性的或者说无法简单合并进投影矩阵而不影响相对位置）。</p><p><strong>MLA 的解决方案</strong>：<br> 它把 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 分成了两部分：</p><ol><li><strong>内容部分 (Content Part)</strong>：参与低秩压缩，<strong>不加 RoPE</strong>。这部分负责语义，享受极致压缩。</li><li><strong>位置部分 (RoPE Part)</strong>：单独保留一个小向量，<strong>加 RoPE</strong>。这部分负责携带位置信息。</li></ol><p>在计算 Attention Score 时，分别计算这两部分的注意力再相加。<br> 虽然这听起来增加了一点计算量，但因为 RoPE 部分的维度很小，增加的计算量微乎其微，却换来了“内容部分”可以被大幅压缩的巨大红利。</p><hr><h3 id="_4-总结-mla-vs-mha-vs-gqa-详细对比" tabindex="-1"><a class="header-anchor" href="#_4-总结-mla-vs-mha-vs-gqa-详细对比"><span>4. 总结：MLA vs MHA vs GQA 详细对比</span></a></h3><table><thead><tr><th style="text-align:left;">特性</th><th style="text-align:left;">MHA (传统)</th><th style="text-align:left;">GQA (主流优化)</th><th style="text-align:left;">MLA (DeepSeek-V2)</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>KV Cache 大小</strong></td><td style="text-align:left;"><strong>极大</strong> (瓶颈)</td><td style="text-align:left;"><strong>中等/小</strong> (约为 MHA 的 1/8)</td><td style="text-align:left;"><strong>极小</strong> (通常优于 MQA，约为 MHA 的 1/50 - 1/100)</td></tr><tr><td style="text-align:left;"><strong>显存占用</strong></td><td style="text-align:left;">高</td><td style="text-align:left;">低</td><td style="text-align:left;"><strong>极低</strong></td></tr><tr><td style="text-align:left;"><strong>模型性能</strong></td><td style="text-align:left;"><strong>最强</strong> (无损)</td><td style="text-align:left;">略有损耗 (参数共享导致)</td><td style="text-align:left;"><strong>强</strong> (理论上优于 GQA，接近 MHA)</td></tr><tr><td style="text-align:left;"><strong>原理核心</strong></td><td style="text-align:left;">独立多头</td><td style="text-align:left;">强制多头共享 KV</td><td style="text-align:left;"><strong>低秩压缩 + 矩阵运算结合律</strong></td></tr><tr><td style="text-align:left;"><strong>推理技巧</strong></td><td style="text-align:left;">无</td><td style="text-align:left;">减少读取量</td><td style="text-align:left;"><strong>吸收 Projection 矩阵，只存压缩隐变量</strong></td></tr></tbody></table><h3 id="_5-通俗类比" tabindex="-1"><a class="header-anchor" href="#_5-通俗类比"><span>5. 通俗类比</span></a></h3><ul><li><strong>MHA (富二代)</strong>：每次出门旅行（推理），给每个朋友（Head）都单独开一辆车（存完整的 KV）。队伍庞大，极其耗油（显存），但在路上每个朋友都能独立看风景（性能好）。</li><li><strong>GQA (拼车族)</strong>：为了省油，强行规定每 8 个朋友必须挤同一辆车（共享 KV）。油省了（显存小了），但有些朋友想去别的地方就不行了（自由度下降，性能略损）。</li><li><strong>MLA (压缩胶囊)</strong>：DeepSeek 发明了一种技术。出门前把所有人的行李和需求压缩成一个小胶囊（Latent Vector）。 <ul><li>到了目的地（计算 Attention）时，不需要把胶囊还原成原本的车队。</li><li>而是让目的地（Query）戴上一副特制的眼镜（吸收了 Up-projection），透过眼镜看这个胶囊，就能直接看到原本那几十辆车队的信息。</li><li><strong>结果</strong>：口袋里只装了一个胶囊（显存极小），但看到的却是完整的豪华车队（性能不降）。</li></ul></li></ul><h3 id="结论" tabindex="-1"><a class="header-anchor" href="#结论"><span>结论</span></a></h3><p>MLA 的改进在于它打破了“显存大小”与“模型性能”之间的权衡（Trade-off）。<br><strong>相比 GQA，MLA 在使用更少显存（KV Cache）的情况下，提供了更强的模型表达能力（类似于 MHA）。</strong></p><p>这也解释了为什么 DeepSeek-V2 作为一个 MoE 模型，能够在保持极高推理速度和极低推理成本的同时，在长上下文任务和复杂逻辑任务中表现出色。</p>',35)])])}const o=a(e,[["render",p]]),h=JSON.parse('{"path":"/blogs/MLA.html","title":"MLA","lang":"zh-CN","frontmatter":{"title":"MLA","date":"2025-12-07T00:00:00.000Z","category":"面试","description":"MLA (Multi-Head Latent Attention，多头潜在注意力机制) 是由 DeepSeek (深度求索) 在其 DeepSeek-V2 模型中提出的一种创新架构。 1. 背景：显存的瓶颈 (KV Cache) 在理解这些机制的区别前，核心要关注的指标是 KV Cache（键值缓存）。 在大模型推理（Generate）阶段，为了避免重...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"MLA\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-12-07T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-07T12:56:17.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"guoziyue-seeker\\",\\"url\\":\\"https://github.com/guoziyue-seeker\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/llm-guide/blogs/MLA.html"}],["meta",{"property":"og:site_name","content":"LLMGuide"}],["meta",{"property":"og:title","content":"MLA"}],["meta",{"property":"og:description","content":"MLA (Multi-Head Latent Attention，多头潜在注意力机制) 是由 DeepSeek (深度求索) 在其 DeepSeek-V2 模型中提出的一种创新架构。 1. 背景：显存的瓶颈 (KV Cache) 在理解这些机制的区别前，核心要关注的指标是 KV Cache（键值缓存）。 在大模型推理（Generate）阶段，为了避免重..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-07T12:56:17.000Z"}],["meta",{"property":"article:published_time","content":"2025-12-07T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-07T12:56:17.000Z"}]]},"git":{"createdTime":1765112177000,"updatedTime":1765112177000,"contributors":[{"name":"guoziyue666","username":"guoziyue666","email":"2567047574@qq.com","commits":1,"url":"https://github.com/guoziyue666"}]},"readingTime":{"minutes":5.43,"words":1628},"filePathRelative":"blogs/MLA.md","excerpt":"<p><strong>MLA (Multi-Head Latent Attention，多头潜在注意力机制)</strong> 是由 <strong>DeepSeek (深度求索)</strong> 在其 <strong>DeepSeek-V2</strong> 模型中提出的一种创新架构。</p>\\n<hr>\\n<h3>1. 背景：显存的瓶颈 (KV Cache)</h3>\\n<p>在理解这些机制的区别前，核心要关注的指标是 <strong>KV Cache（键值缓存）</strong>。<br>\\n在大模型推理（Generate）阶段，为了避免重复计算，我们会把之前生成的 Token 的 Key 和 Value 矩阵存在显存里。</p>","autoDesc":true}');export{o as comp,h as data};
