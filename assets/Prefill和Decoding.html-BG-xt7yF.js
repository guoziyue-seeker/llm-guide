import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,d as n,o as e}from"./app-alnVQ7pP.js";const l={};function r(i,s){return e(),a("div",null,[...s[0]||(s[0]=[n('<p>在大模型（LLM）推理过程中，**Prefill（预填充/首字生成）**和 <strong>Decoding（解码/逐字生成）</strong> 是两个截然不同的计算阶段。</p><p>虽然它们用的是同一个模型、同一套权重，但在<strong>计算形态</strong>、<strong>硬件瓶颈</strong>和<strong>数据流动方式</strong>上，它们简直像是两种不同的负载。</p><p>以下是详细的深度对比：</p><hr><h3 id="_1-核心定义与任务" tabindex="-1"><a class="header-anchor" href="#_1-核心定义与任务"><span>1. 核心定义与任务</span></a></h3><ul><li><strong>Prefill (预填充阶段)</strong><ul><li><strong>任务：</strong> “阅读理解”。模型接收你输入的 Prompt（可能几千字），一次性并行处理所有输入的 Token，计算出它们的相互关系，并生成<strong>第一个</strong>输出 Token。</li><li><strong>特点：</strong> <strong>并行计算 (Parallel)</strong>。</li></ul></li><li><strong>Decoding (解码阶段)</strong><ul><li><strong>任务：</strong> “逐字生成”。基于前面的 Prompt 和已经生成的字，计算<strong>下一个</strong>字。这是一个自回归过程（Autoregressive），必须等上一个字出结果，才能算下一个。</li><li><strong>特点：</strong> <strong>串行计算 (Serial)</strong>。</li></ul></li></ul><hr><h3 id="_2-数学运算形态-gemm-vs-gemv" tabindex="-1"><a class="header-anchor" href="#_2-数学运算形态-gemm-vs-gemv"><span>2. 数学运算形态：GEMM vs GEMV</span></a></h3><p>这是两者在 GPU 内部计算时最本质的区别。</p><h4 id="prefill-矩阵乘矩阵-gemm-general-matrix-matrix-multiplication" tabindex="-1"><a class="header-anchor" href="#prefill-矩阵乘矩阵-gemm-general-matrix-matrix-multiplication"><span>Prefill：矩阵乘矩阵 (GEMM - General Matrix-Matrix Multiplication)</span></a></h4><ul><li>假设你输入了 512 个 Token（Sequence Length <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">N=512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span>），模型隐藏层维度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>=</mo><mn>4096</mn></mrow><annotation encoding="application/x-tex">D=4096</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4096</span></span></span></span>。</li><li>在计算每一层时，输入数据是一个巨大的矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>512</mn><mo separator="true">,</mo><mn>4096</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[512, 4096]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">512</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">4096</span><span class="mclose">]</span></span></span></span>。</li><li>它要和模型权重矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>4096</mn><mo separator="true">,</mo><mn>4096</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[4096, 4096]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">4096</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">4096</span><span class="mclose">]</span></span></span></span> 相乘。</li><li><strong>计算特点：</strong> 这是一个**“胖”矩阵<strong>乘以</strong>“胖”矩阵**。</li><li><strong>算术强度 (Arithmetic Intensity)：</strong> <strong>高</strong>。 <ul><li>你从 HBM 搬运一次权重（比如 100GB），可以用这 100GB 权重去乘 512 行数据。搬运一次，干了 512 份活。</li></ul></li></ul><h4 id="decoding-矩阵乘向量-gemv-general-matrix-vector-multiplication" tabindex="-1"><a class="header-anchor" href="#decoding-矩阵乘向量-gemv-general-matrix-vector-multiplication"><span>Decoding：矩阵乘向量 (GEMV - General Matrix-Vector Multiplication)</span></a></h4><ul><li>现在只生成 1 个 Token（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>）。</li><li>输入数据只是一个向量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mn>4096</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[1, 4096]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">4096</span><span class="mclose">]</span></span></span></span>。</li><li>它依然要和巨大的模型权重矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>4096</mn><mo separator="true">,</mo><mn>4096</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[4096, 4096]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">4096</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">4096</span><span class="mclose">]</span></span></span></span> 相乘。</li><li><strong>计算特点：</strong> 这是一个**“扁”向量<strong>乘以</strong>“胖”矩阵**。</li><li><strong>算术强度：</strong> <strong>极低</strong>。 <ul><li>你依然需要从 HBM 搬运那 100GB 的权重，但这次搬过来只为了乘 1 行数据。搬运一次，只干了 1 份活。</li></ul></li></ul><hr><h3 id="_3-硬件瓶颈-计算受限-vs-内存受限" tabindex="-1"><a class="header-anchor" href="#_3-硬件瓶颈-计算受限-vs-内存受限"><span>3. 硬件瓶颈：计算受限 vs 内存受限</span></a></h3><p>基于上面的数学形态，两者对硬件的压力点完全不同。</p><h4 id="prefill-是-compute-bound-算力受限" tabindex="-1"><a class="header-anchor" href="#prefill-是-compute-bound-算力受限"><span>Prefill 是 Compute-bound (算力受限)</span></a></h4><ul><li><strong>瓶颈在 GPU 核心 (Tensor Cores)</strong>。</li><li>因为一次性处理的数据量很大（Batch Size 大），GPU 的计算单元被喂得很饱。</li><li>这时候，显存带宽（HBM）通常不是主要问题，计算单元算得冒烟了才是瓶颈。</li><li><strong>优化方向：</strong> 需要更强的 FLOPS（每秒浮点运算次数），比如 H100 的 FP8 算力。</li></ul><h4 id="decoding-是-memory-bound-显存带宽受限" tabindex="-1"><a class="header-anchor" href="#decoding-是-memory-bound-显存带宽受限"><span>Decoding 是 Memory-bound (显存带宽受限)</span></a></h4><ul><li><strong>瓶颈在 显存带宽 (HBM Bandwidth)</strong>。</li><li>GPU 核心大部分时间在**“等”**。等数据从 HBM 搬到 SRAM。数据一到，核心瞬间（几纳秒）就把它算完了，然后继续等下一批权重数据。</li><li><strong>算力浪费严重：</strong> 在 Decoding 阶段，RTX 4090 的算力利用率可能连 1% 都不到，因为它一直在空转等数据。</li><li><strong>优化方向：</strong> 需要更大的显存带宽（HBM3e），或者量化技术（把权重变小，搬运更快）。</li></ul><hr><h3 id="_4-kv-cache-的行为差异" tabindex="-1"><a class="header-anchor" href="#_4-kv-cache-的行为差异"><span>4. KV Cache 的行为差异</span></a></h3><p>KV Cache 是大模型的“记忆”，存储了之前所有 Token 的 Key 和 Value 矩阵。</p><ul><li><strong>在 Prefill 阶段：</strong><ul><li><strong>行为：</strong> <strong>生产 (Create)</strong>。</li><li>模型一次性算出 Prompt 中所有 512 个 Token 的 K 和 V，并把它们写入显存。这是 KV Cache 增长最快的一瞬间。</li></ul></li><li><strong>在 Decoding 阶段：</strong><ul><li><strong>行为：</strong> <strong>检索 + 追加 (Retrieve + Append)</strong>。</li><li>为了生成第 513 个字，模型需要去显存里把前 512 个字的 KV Cache <strong>全部读出来</strong>（与当前的 Query 做 Attention），算完后，把第 513 个字的 K 和 V <strong>追加</strong>写入显存。</li><li><strong>随着对话变长，Decoding 会越来越慢</strong>，因为每一轮要读取的 KV Cache 数据量在不断增加。</li></ul></li></ul><hr><h3 id="_5-形象的比喻-考试" tabindex="-1"><a class="header-anchor" href="#_5-形象的比喻-考试"><span>5. 形象的比喻：考试</span></a></h3><ul><li><p><strong>Prefill 就像是“阅读试卷材料”：</strong></p><ul><li>你需要看一篇 2000 字的文章。你可以<strong>一眼看十行</strong>（并行），大脑飞速运转理解文意。</li><li>这时候你的大脑（算力）是满载的，眼睛（带宽）只要正常扫视就行。</li></ul></li><li><p><strong>Decoding 就像是“写作文”：</strong></p><ul><li>你必须<strong>一个字一个字地写</strong>（串行）。</li><li>而在写每一个字之前，你必须<strong>重新回顾一遍整篇文章</strong>（读取全部权重和 KV Cache）来决定下一个字写什么。</li><li>虽然写一个字很快（计算快），但“回顾全文”的过程很花时间（带宽慢）。如果文章很长，你大部分时间都在翻试卷（搬运数据），而不是在写字。</li></ul></li></ul><hr><h3 id="总结对比表" tabindex="-1"><a class="header-anchor" href="#总结对比表"><span>总结对比表</span></a></h3><table><thead><tr><th style="text-align:left;">维度</th><th style="text-align:left;">Prefill (预填充)</th><th style="text-align:left;">Decoding (解码)</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>输入形状</strong></td><td style="text-align:left;">Batch x Sequence Length (大)</td><td style="text-align:left;">Batch x 1 (小)</td></tr><tr><td style="text-align:left;"><strong>计算类型</strong></td><td style="text-align:left;"><strong>GEMM</strong> (矩阵 x 矩阵)</td><td style="text-align:left;"><strong>GEMV</strong> (矩阵 x 向量)</td></tr><tr><td style="text-align:left;"><strong>瓶颈</strong></td><td style="text-align:left;"><strong>Compute-bound</strong> (拼算力)</td><td style="text-align:left;"><strong>Memory-bound</strong> (拼带宽)</td></tr><tr><td style="text-align:left;"><strong>GPU利用率</strong></td><td style="text-align:left;">高 (接近 100%)</td><td style="text-align:left;">低 (核心大量空转)</td></tr><tr><td style="text-align:left;"><strong>延迟敏感度</strong></td><td style="text-align:left;">首字延迟 (TTFT)</td><td style="text-align:left;">字符间延迟 (TPOT)</td></tr><tr><td style="text-align:left;"><strong>Attention 行为</strong></td><td style="text-align:left;">计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>×</mo><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">Q \\times K^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span> 产生 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">N \\times N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 矩阵</td><td style="text-align:left;">Query 向量与所有历史 K 缓存交互</td></tr><tr><td style="text-align:left;"><strong>优化技术</strong></td><td style="text-align:left;">FlashAttention</td><td style="text-align:left;">FlashDecoding, 量化 (Quantization)</td></tr></tbody></table><p><strong>这就是为什么显卡厂商宣传算力（FLOPS）时通常是在吹 Prefill 的能力；而我们在本地部署大模型感到“卡顿、出字慢”时，通常是因为显存带宽限制了 Decoding 的速度。</strong></p>',31)])])}const m=t(l,[["render",r]]),g=JSON.parse('{"path":"/blogs/Prefill%E5%92%8CDecoding.html","title":"Prefill和Decoding","lang":"zh-CN","frontmatter":{"title":"Prefill和Decoding","date":"2025-12-07T00:00:00.000Z","category":"面试","description":"在大模型（LLM）推理过程中，**Prefill（预填充/首字生成）**和 Decoding（解码/逐字生成） 是两个截然不同的计算阶段。 虽然它们用的是同一个模型、同一套权重，但在计算形态、硬件瓶颈和数据流动方式上，它们简直像是两种不同的负载。 以下是详细的深度对比： 1. 核心定义与任务 Prefill (预填充阶段) 任务： “阅读理解”。模型接...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Prefill和Decoding\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-12-07T00:00:00.000Z\\",\\"dateModified\\":\\"2025-12-07T10:02:00.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"guoziyue-seeker\\",\\"url\\":\\"https://github.com/guoziyue-seeker\\"}]}"],["meta",{"property":"og:url","content":"https://mister-hope.github.io/llm-guide/blogs/Prefill%E5%92%8CDecoding.html"}],["meta",{"property":"og:site_name","content":"LLMGuide"}],["meta",{"property":"og:title","content":"Prefill和Decoding"}],["meta",{"property":"og:description","content":"在大模型（LLM）推理过程中，**Prefill（预填充/首字生成）**和 Decoding（解码/逐字生成） 是两个截然不同的计算阶段。 虽然它们用的是同一个模型、同一套权重，但在计算形态、硬件瓶颈和数据流动方式上，它们简直像是两种不同的负载。 以下是详细的深度对比： 1. 核心定义与任务 Prefill (预填充阶段) 任务： “阅读理解”。模型接..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-07T10:02:00.000Z"}],["meta",{"property":"article:published_time","content":"2025-12-07T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-07T10:02:00.000Z"}]]},"git":{"createdTime":1765101720000,"updatedTime":1765101720000,"contributors":[{"name":"guoziyue666","username":"guoziyue666","email":"2567047574@qq.com","commits":1,"url":"https://github.com/guoziyue666"}]},"readingTime":{"minutes":4.39,"words":1318},"filePathRelative":"blogs/Prefill和Decoding.md","excerpt":"<p>在大模型（LLM）推理过程中，**Prefill（预填充/首字生成）**和 <strong>Decoding（解码/逐字生成）</strong> 是两个截然不同的计算阶段。</p>\\n<p>虽然它们用的是同一个模型、同一套权重，但在<strong>计算形态</strong>、<strong>硬件瓶颈</strong>和<strong>数据流动方式</strong>上，它们简直像是两种不同的负载。</p>\\n<p>以下是详细的深度对比：</p>\\n<hr>\\n<h3>1. 核心定义与任务</h3>\\n<ul>\\n<li><strong>Prefill (预填充阶段)</strong>\\n<ul>\\n<li><strong>任务：</strong> “阅读理解”。模型接收你输入的 Prompt（可能几千字），一次性并行处理所有输入的 Token，计算出它们的相互关系，并生成<strong>第一个</strong>输出 Token。</li>\\n<li><strong>特点：</strong> <strong>并行计算 (Parallel)</strong>。</li>\\n</ul>\\n</li>\\n<li><strong>Decoding (解码阶段)</strong>\\n<ul>\\n<li><strong>任务：</strong> “逐字生成”。基于前面的 Prompt 和已经生成的字，计算<strong>下一个</strong>字。这是一个自回归过程（Autoregressive），必须等上一个字出结果，才能算下一个。</li>\\n<li><strong>特点：</strong> <strong>串行计算 (Serial)</strong>。</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}');export{m as comp,g as data};
