---
title: Online Softmax
date: 2025-12-07
category: 面试
---

**Online Softmax** 是 FlashAttention 能够成功的数学基石。

要理解它，我们得先看**传统的 Softmax** 在计算机里是怎么算的，遇到了什么问题，然后看 Online Softmax 是如何用一个巧妙的**“数学补丁”**解决这个问题的。

---

### 1. 传统 Softmax 的“三步走”困境

Softmax 的公式原本很简单：
$$ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum e^{x_j}} $$

但在计算机里，直接算 $e^{x}$ 会出事。如果 $x$ 很大（比如 100），$e^{100}$ 会变成一个天文数字，导致**数据溢出（Overflow/NaN）**。

所以，工程上通用的做法是 **Safe Softmax**，它分为三步：

1.  **找老大（Find Max）：** 遍历整个向量 $x$，找到最大值 $m$。
2.  **算总和（Calculate Sum）：** 算出分母。为了防止溢出，每个人都先减去最大值 $m$ 再求指数和。
    $$ d = \sum_{j} e^{x_j - m} $$
3.  **算结果（Compute Output）：**
    $$ \text{Softmax}(x_i) = \frac{e^{x_i - m}}{d} $$

**痛点：**
你需要**遍历数据 3 次**（找最大值 -> 算分母 -> 算除法）。
这意味着如果你要把向量切片（Tiling）放进 SRAM 算，你做不到。因为你在处理第一块数据时，**不知道后面的数据里有没有比当前更大的 $m$**。如果后面出现了新的“老大”，前面的计算全废了。

---

### 2. Online Softmax 的核心魔法：动态修正

Online Softmax 的核心思想是：**我不等看完全局，我走一步算一步。如果后面发现了新的最大值，我就用数学公式把前面的结果“修正”一下。**

假设我们把长向量切成两段处理：**Block 1** 和 **Block 2**。

#### 第一步：处理 Block 1
我们在 SRAM 里处理第一块数据。
1.  找到局部最大值 $m_1$。
2.  算出局部指数和 $d_1 = \sum e^{x - m_1}$。

#### 第二步：处理 Block 2（关键时刻）
现在读入第二块数据。
1.  找到 Block 2 的局部最大值 $m_2$。
2.  算出 Block 2 的局部指数和 $d_2 = \sum e^{x - m_2}$。

现在我们需要合并结果，得到**全局**的最大值和总和。
*   **全局最大值：** $m_{new} = \max(m_1, m_2)$。这很简单。
*   **全局总和：** 这里有问题。
    *   $d_1$ 是基于 $m_1$ 算的。
    *   $d_2$ 是基于 $m_2$ 算的。
    *   **不能直接相加！** 基准不一样。

**修正公式（Rescaling）：**
我们需要把 $d_1$ 的基准从 $m_1$ 换算成 $m_{new}$。
数学推导如下：
$$ e^{x - m_{new}} = e^{x - m_1 + m_1 - m_{new}} = e^{x - m_1} \cdot e^{m_1 - m_{new}} $$

所以，新的全局总和 $d_{new}$ 等于：
$$ d_{new} = d_1 \cdot \underbrace{e^{m_1 - m_{new}}}_{\text{修正系数}} + d_2 \cdot \underbrace{e^{m_2 - m_{new}}}_{\text{修正系数}} $$

*   如果 $m_{new} = m_2$，那么 $d_1$ 就会乘以一个小于 1 的系数（因为 $m_1 - m_{new}$ 是负数），相当于**“贬值”**了；而 $d_2$ 保持不变（指数为0，系数为1）。

---

### 3. 数值举例（小学生都能看懂版）

假设我们要算向量 `[1, 3, 5, 2]` 的 Softmax 分母。我们将它切成两块：`[1, 3]` 和 `[5, 2]`。

#### 第一轮：处理 `[1, 3]`
1.  **局部最大值 $m_1$** = 3。
2.  **局部总和 $d_1$** = $e^{1-3} + e^{3-3} = e^{-2} + e^0 \approx 0.135 + 1 = 1.135$。
    *(此时我们暂时认为全局最大是 3，总和是 1.135)*

#### 第二轮：处理 `[5, 2]`
1.  **局部最大值 $m_2$** = 5。
2.  **局部总和 $d_2$** = $e^{5-5} + e^{2-5} = e^0 + e^{-3} \approx 1 + 0.05 = 1.05$。
3.  **发现新老大：** 全局最大值 $m_{new} = \max(3, 5) = 5$。

#### 第三轮：合并修正
我们需要更新 $d_1$，让它承认“5”才是老大。
*   **修正系数** = $e^{m_{old} - m_{new}} = e^{3 - 5} = e^{-2} \approx 0.135$。
*   **旧总和修正** = $d_1 \times 0.135 = 1.135 \times 0.135 \approx 0.153$。
    *(这其实就是 `[1, 3]` 这两个数相对于最大值 5 的指数和)*
*   **全局新总和** = 修正后的旧总和 + $d_2$
    = $0.153 + 1.05 = 1.203$。

**验证：**
如果我们直接算全局 `[1, 3, 5, 2]`：
Max = 5。
Sum = $e^{1-5} + e^{3-5} + e^{5-5} + e^{2-5} \approx 0.018 + 0.135 + 1 + 0.05 = 1.203$。
**结果完全一致！**

---

### 4. 进阶：不仅仅是算分母 (FlashAttention 的核心)

FlashAttention 不仅仅是算 Softmax 的分母，它还要算最终的 Attention Output ($O = \text{Softmax}(QK^T)V$)。

Result 矩阵 $O$ 也是在 SRAM 里分块累加的。当最大值 $m$ 发生变化时，**之前算出来的 Result 矩阵 $O$ 也要跟着贬值！**

FlashAttention 的迭代更新公式如下（简化版）：

1.  从 SRAM 拿到旧的 Result ($O_{old}$)，旧的 Sum ($d_{old}$)，旧的 Max ($m_{old}$)。
2.  计算当前块的新 Max ($m_{curr}$) 和新 Sum ($d_{curr}$)。
3.  确定新的一轮全局 Max: $m_{new} = \max(m_{old}, m_{curr})$。
4.  **同时更新 Result 矩阵：**
    $$ O_{new} = \text{diag}(e^{m_{old} - m_{new}}) \cdot O_{old} + e^{m_{curr} - m_{new}} \cdot P_{curr} V_{curr} $$

**翻译成人话：**
*   把之前辛辛苦苦算出来的结果 $O_{old}$，根据“新旧最大值的差”，乘一个缩小系数（Rescale）。
*   加上当前这一块新算出来的结果（也要根据它和最大值的差进行缩放）。
*   除以新的总权重（Normalization）。

### 5. 总结

**Online Softmax 实际上是把“除法归一化”这个动作推迟了。**

*   **传统做法：** 看完全部 -> 找最大 -> 算完所有指数和 -> 做除法。
*   **Online 做法：**
    *   我看一块，算一块的指数。
    *   我维护一个“未归一化”的分子总和（Running Sum）和一个分母（Normalizer）。
    *   每当发现更大的数，我就把手里的分子和分母都按比例“缩水”（Rescale），保证它们是基于同一个基准。
    *   等所有块都看完了，最后做一次除法。

正是因为有了这个算法，FlashAttention 才能做到**把 Q, K, V 切成小块放入 SRAM，算完一块扔一块，中间不需要把巨大的 $N \times N$ 矩阵写回显存**，从而实现了速度和显存的双重飞跃。