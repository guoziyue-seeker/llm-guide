---
title: flash attention
date: 2025-12-07
category: 博客
---

**一种通过“减少 HBM 访问次数”来加速计算的算法。它不改变计算量（FLOPS），但极大地减少了数据搬运的时间（IO）**

简单一句话总结 FlashAttention 的核心思想：
**它是一种通过“减少 HBM 访问次数”来加速计算的算法。它不改变计算量（FLOPS），但极大地减少了数据搬运的时间（IO）。**

在大模型领域，**IO（搬运数据）往往比计算更慢**，所以“少跑几趟仓库”就能带来巨大的速度提升。

---

### 1. 传统 Attention 的痛点：HBM 的“反复折返跑”

在标准的 Attention 算法（即 Transformers 的核心公式 $\text{Softmax}(QK^T)V$）中，计算过程是非常“铺张浪费”的。

假设我们有一个长文本序列，长度为 $N$。
1.  **$Q \times K^T$：** GPU 把 Q 和 K 从 HBM 读进 SRAM 算乘法，生成一个 $N \times N$ 的巨大矩阵（Attention Score）。
    *   *痛点：* SRAM 太小装不下这个大矩阵，所以算完必须**写回 HBM**。
2.  **Softmax：** GPU 把刚才那个大矩阵从 HBM 读回来，做 Softmax 归一化。
    *   *痛点：* 算完又要**写回 HBM**。
3.  **$\times V$：** GPU 再次把归一化后的矩阵和 V 从 HBM 读出来，做最后乘法。
    *   *痛点：* 算完结果**写回 HBM**。

**结果：**
*   **显存爆炸：** 那个中间产生的 $N \times N$ 矩阵随着序列长度变长，体积呈平方级增长（$O(N^2)$）。如果你输入 100k 字的文档，这个中间矩阵直接把显存撑爆（OOM）。
*   **带宽瓶颈：** 数据在 HBM 和 SRAM 之间来回搬运了 3 次以上。GPU 核心大部分时间在等 HBM 搬数据，而不是在计算。

---

### 2. FlashAttention 的魔法：切块（Tiling）与 算子融合

FlashAttention 的发明者（Tri Dao）意识到：**既然 SRAM 虽小但极快，那我们为什么不把所有事情都在 SRAM 里一次性做完再出来？**

#### 核心操作 A：分块计算 (Tiling)
FlashAttention 不会试图一次性算出整个 $N \times N$ 的大矩阵。
它把 Q, K, V 切成很多小块（Block）。比如，每次只切一小块 Q 和一小块 K 放进 SRAM。

#### 核心操作 B：算子融合 (Kernel Fusion)
在 SRAM 内部，它一口气完成以下动作：
1.  算一小块 $Q \times K^T$。
2.  **立即**在 SRAM 里对这一小块数据做 Softmax（配合 Online Softmax 技术，下面会讲）。
3.  **立即**用这一小块结果去乘 V。
4.  只把最终计算好的那**一小部分结果**写回 HBM。

**关键改变：** 那个巨大的 $N \times N$ 中间矩阵根本不需要在 HBM 里生成和存储！它被“拆碎”在 SRAM 里消化掉了。

---

### 3. 数学上的难点：Online Softmax

你可能会问：*“Softmax 不是需要看全行数据才能算概率吗？切成小块怎么算？”*

比如一行数据是 `[2, 3, 5]`。
*   传统 Softmax：必须知道最大值是 5，总和是多少，才能算出每个数的概率。如果只看第一块 `[2]`，根本不知道后面还有个 `5`。

**FlashAttention 引入了“Online Softmax”技巧：**

*   它在处理第一块时，先假设当前看到的是最大值，算一个临时结果。
*   当读到第二块发现更大的值时，利用数学公式对之前的临时结果进行**修正（Rescaling）**。
*   这样就可以一边读块，一边更新结果，不需要一次性看到全局。

---

### 4. 对“推理”的具体优化作用

大模型推理分为两个阶段，FlashAttention 的作用略有不同：

#### 第一阶段：Prefill（预填充/处理提示词）
这是你把一大段 Prompt 发给模型，模型阅读理解的阶段。
*   **场景特点：** 这是一个高度并行的矩阵计算，和训练过程很像。
*   **FlashAttention 的作用：** **极度显著**。
    *   **速度：** 比如你发了 10 万字的文档让 Kimi/GPT-4 总结。没有 FlashAttention，处理这 10 万字可能需要几分钟；有了它，只需几秒。因为它避免了 $N \times N$ 矩阵的读写。
    *   **显存：** 使得长文本推理成为可能。它把显存占用从 $O(N^2)$ 降到了 $O(N)$（线性增长）。如果没有它，消费级显卡根本跑不了长窗口模型。

#### 第二阶段：Decoding（解码/逐字生成）
这是模型一个字一个字往外蹦的阶段。
*   **场景特点：** 主要是向量乘矩阵（Vector-Matrix Multiply）。每次只生成 1 个 Token，Q 只有 1 行，但 KV Cache 很长。
*   **FlashAttention 的作用：** **FlashDecoding**。
    *   早期的 FlashAttention 在这里主要优化了显存占用。
    *   后来的变体 **FlashDecoding** 专门针对推理生成阶段优化：它通过并行加载 KV Cache 的不同部分来加速 Attention 计算。
    *   **结果：** 让生成速度（Tokens/s）在长上下文时不掉速。以前 context 越长生成越慢，现在基本能保持恒定高速。

---

### 5. 厨房类比（终极版）

为了对应之前的 SRAM/HBM 概念：

*   **做一道极其复杂的菜（计算 Attention）：** 需要切菜、焯水、过油、勾芡。
*   **传统做法（Standard Attention）：**
    1.  从冰箱（HBM）拿出 100 斤土豆。
    2.  切完，因为案板（SRAM）放不下，把 100 斤切好的土豆放回冰箱。
    3.  从冰箱拿出切好的土豆，焯水，再放回冰箱。
    4.  从冰箱拿出焯水的土豆，过油……
    *   **问题：** 厨师大部分时间都在跑腿开冰箱门，而且冰箱快塞不下了。

*   **FlashAttention 做法：**
    1.  从冰箱只拿 1 个土豆。
    2.  在案板上（SRAM）瞬间完成切、焯、炒、勾芡。
    3.  直接把这一小盘成品端上桌（写入最终结果）。
    4.  再去拿下一个土豆。
    *   **优势：** 厨师不用跑腿，冰箱也不会被半成品塞满。

### 总结

FlashAttention 对大模型推理的贡献在于：
1.  **打破了内存墙：** 通过减少 HBM 访问，极大提升了推理速度（尤其是长文本处理）。
2.  **拯救了显存：** 不再存储巨大的中间矩阵，让有限的显存能处理几十倍长度的 Context Window（比如从 4k 扩展到 128k 甚至 1M）。