---
title: Prefill和Decoding
date: 2025-12-07
category: 面试
---

在大模型（LLM）推理过程中，**Prefill（预填充/首字生成）**和 **Decoding（解码/逐字生成）** 是两个截然不同的计算阶段。

虽然它们用的是同一个模型、同一套权重，但在**计算形态**、**硬件瓶颈**和**数据流动方式**上，它们简直像是两种不同的负载。

以下是详细的深度对比：

---

### 1. 核心定义与任务

*   **Prefill (预填充阶段)**
    *   **任务：** “阅读理解”。模型接收你输入的 Prompt（可能几千字），一次性并行处理所有输入的 Token，计算出它们的相互关系，并生成**第一个**输出 Token。
    *   **特点：** **并行计算 (Parallel)**。
*   **Decoding (解码阶段)**
    *   **任务：** “逐字生成”。基于前面的 Prompt 和已经生成的字，计算**下一个**字。这是一个自回归过程（Autoregressive），必须等上一个字出结果，才能算下一个。
    *   **特点：** **串行计算 (Serial)**。

---

### 2. 数学运算形态：GEMM vs GEMV

这是两者在 GPU 内部计算时最本质的区别。

#### Prefill：矩阵乘矩阵 (GEMM - General Matrix-Matrix Multiplication)
*   假设你输入了 512 个 Token（Sequence Length $N=512$），模型隐藏层维度 $D=4096$。
*   在计算每一层时，输入数据是一个巨大的矩阵 $[512, 4096]$。
*   它要和模型权重矩阵 $[4096, 4096]$ 相乘。
*   **计算特点：** 这是一个**“胖”矩阵**乘以**“胖”矩阵**。
*   **算术强度 (Arithmetic Intensity)：** **高**。
    *   你从 HBM 搬运一次权重（比如 100GB），可以用这 100GB 权重去乘 512 行数据。搬运一次，干了 512 份活。

#### Decoding：矩阵乘向量 (GEMV - General Matrix-Vector Multiplication)
*   现在只生成 1 个 Token（$N=1$）。
*   输入数据只是一个向量 $[1, 4096]$。
*   它依然要和巨大的模型权重矩阵 $[4096, 4096]$ 相乘。
*   **计算特点：** 这是一个**“扁”向量**乘以**“胖”矩阵**。
*   **算术强度：** **极低**。
    *   你依然需要从 HBM 搬运那 100GB 的权重，但这次搬过来只为了乘 1 行数据。搬运一次，只干了 1 份活。

---

### 3. 硬件瓶颈：计算受限 vs 内存受限

基于上面的数学形态，两者对硬件的压力点完全不同。

#### Prefill 是 Compute-bound (算力受限)
*   **瓶颈在 GPU 核心 (Tensor Cores)**。
*   因为一次性处理的数据量很大（Batch Size 大），GPU 的计算单元被喂得很饱。
*   这时候，显存带宽（HBM）通常不是主要问题，计算单元算得冒烟了才是瓶颈。
*   **优化方向：** 需要更强的 FLOPS（每秒浮点运算次数），比如 H100 的 FP8 算力。

#### Decoding 是 Memory-bound (显存带宽受限)
*   **瓶颈在 显存带宽 (HBM Bandwidth)**。
*   GPU 核心大部分时间在**“等”**。等数据从 HBM 搬到 SRAM。数据一到，核心瞬间（几纳秒）就把它算完了，然后继续等下一批权重数据。
*   **算力浪费严重：** 在 Decoding 阶段，RTX 4090 的算力利用率可能连 1% 都不到，因为它一直在空转等数据。
*   **优化方向：** 需要更大的显存带宽（HBM3e），或者量化技术（把权重变小，搬运更快）。

---

### 4. KV Cache 的行为差异

KV Cache 是大模型的“记忆”，存储了之前所有 Token 的 Key 和 Value 矩阵。

*   **在 Prefill 阶段：**
    *   **行为：** **生产 (Create)**。
    *   模型一次性算出 Prompt 中所有 512 个 Token 的 K 和 V，并把它们写入显存。这是 KV Cache 增长最快的一瞬间。
*   **在 Decoding 阶段：**
    *   **行为：** **检索 + 追加 (Retrieve + Append)**。
    *   为了生成第 513 个字，模型需要去显存里把前 512 个字的 KV Cache **全部读出来**（与当前的 Query 做 Attention），算完后，把第 513 个字的 K 和 V **追加**写入显存。
    *   **随着对话变长，Decoding 会越来越慢**，因为每一轮要读取的 KV Cache 数据量在不断增加。

---

### 5. 形象的比喻：考试

*   **Prefill 就像是“阅读试卷材料”：**
    *   你需要看一篇 2000 字的文章。你可以**一眼看十行**（并行），大脑飞速运转理解文意。
    *   这时候你的大脑（算力）是满载的，眼睛（带宽）只要正常扫视就行。

*   **Decoding 就像是“写作文”：**
    *   你必须**一个字一个字地写**（串行）。
    *   而在写每一个字之前，你必须**重新回顾一遍整篇文章**（读取全部权重和 KV Cache）来决定下一个字写什么。
    *   虽然写一个字很快（计算快），但“回顾全文”的过程很花时间（带宽慢）。如果文章很长，你大部分时间都在翻试卷（搬运数据），而不是在写字。

---

### 总结对比表

| 维度               | Prefill (预填充)                           | Decoding (解码)                    |
| :----------------- | :----------------------------------------- | :--------------------------------- |
| **输入形状**       | Batch x Sequence Length (大)               | Batch x 1 (小)                     |
| **计算类型**       | **GEMM** (矩阵 x 矩阵)                     | **GEMV** (矩阵 x 向量)             |
| **瓶颈**           | **Compute-bound** (拼算力)                 | **Memory-bound** (拼带宽)          |
| **GPU利用率**      | 高 (接近 100%)                             | 低 (核心大量空转)                  |
| **延迟敏感度**     | 首字延迟 (TTFT)                            | 字符间延迟 (TPOT)                  |
| **Attention 行为** | 计算 $Q \times K^T$ 产生 $N \times N$ 矩阵 | Query 向量与所有历史 K 缓存交互    |
| **优化技术**       | FlashAttention                             | FlashDecoding, 量化 (Quantization) |

**这就是为什么显卡厂商宣传算力（FLOPS）时通常是在吹 Prefill 的能力；而我们在本地部署大模型感到“卡顿、出字慢”时，通常是因为显存带宽限制了 Decoding 的速度。**